{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cherrickunh/lakeCoSTR_colab/blob/main/lakeCoSTRv1_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr9ipVEDv7BK"
      },
      "source": [
        "# **lakeCoSTR for Colab**<br>\n",
        "a tool for *__lake__ __Co__ llection 2 __S__ urface __T__ emperature __R__ etrieval*<p>\n",
        "<p>\n",
        "\n",
        "**Version 1.13**<br>\n",
        "*Last updated: 2023-03-22<p>*\n",
        "\n",
        "Created by:<br>\n",
        "Christina Herrick<br>\n",
        "University of New Hampshire<br>\n",
        "christina.herrick@unh.edu<p>\n",
        "Bethel Steele<br>\n",
        "Cary Institute of Ecosystem Studies<br>\n",
        "steeleb@caryinstitute.org<p>\n",
        "__This tool is still under development and has an associated manuscript - https://doi.org/10.1002/ecs2.4357. This tool is licensed under CC-BY-NC-SA, please cite accordingly.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hj_-5o-9Fsz"
      },
      "source": [
        "# **User Guide**\n",
        "Please review the [**user guide**](https://github.com/lakeCoSTR/lakeCoSTR_colab/blob/main/UserGuide_lakeCoSTR_colab.md) for assistance in navigating the tool and for specific constraints and caveats.\n",
        "\n",
        "# **Motivations of lakeCoSTR**\n",
        "We created lakeCoSTR to make the acquisition and analysis of Landsat's Collection 2 Surface Temperature product more accessible for those interested in obtaining historical estimates of lake temperature from remote sensing data. This tool is geared to researchers interested in gathering historical temperature estimates, especially for small lakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psnrRlTjBj2M"
      },
      "source": [
        "# **1. Initial Setup**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3zRra91DaEq"
      },
      "source": [
        "## **1.1 Modules**\n",
        "\n",
        "This section of code blocks imports necessary python modules for the notebook to run. You will be prompted to click one or more URLs and be taken to a page to sign in with your Google account.\n",
        "\n",
        "Copy the unique code(s) provided on the web page when prompted and paste where prompted to finish authorization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gh-VYdMKCAkM"
      },
      "outputs": [],
      "source": [
        "#@markdown __Run this block__ to authorize Colab to authenticate your Google account and give it access to upload files from your local computer.\n",
        "from google.colab import auth, files\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "STd1_lFcVYMZ"
      },
      "outputs": [],
      "source": [
        "#@markdown __Run this block__ to connect Colab to your Earth Engine account\n",
        "#@markdown You must already be authorized to use Google Earth Engine. \n",
        "#@markdown <p>If you do not already have access, <a href=\"https://signup.earthengine.google.com/#!/\" target=\"_blank\">fill out this application</a>.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SaB_g_reJG41"
      },
      "outputs": [],
      "source": [
        "#@markdown __Run this block__ to connect Colab to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8SN2nSfHBF1S"
      },
      "outputs": [],
      "source": [
        "#@markdown __Run this block__ to install packages and functions used for \n",
        "#@markdown interactive mapping and data exploration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "from time import strftime, sleep\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import folium\n",
        "from folium import plugins\n",
        "from google.colab import data_table\n",
        "\n",
        "try:\n",
        "  from ipyleaflet import Map, DrawControl\n",
        "  from ipywidgets import Layout\n",
        "except ImportError:\n",
        "  subprocess.check_call([\"python\",\"-m\",\"pip\",\"install\",\"-U\",\"ipyleaflet\"])\n",
        "  from ipyleaflet import Map, basemaps, basemap_to_tiles, DrawControl\n",
        "  from ipywidgets import Layout\n",
        "\n",
        "# Add custom basemaps to folium\n",
        "basemaps = {\n",
        "    'Google Maps': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=m&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Maps',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Google Satellite': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Satellite',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Google Terrain': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=p&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Terrain',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Google Satellite Hybrid': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Satellite',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Esri Satellite': folium.TileLayer(\n",
        "        tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
        "        attr = 'Esri',\n",
        "        name = 'Esri Satellite',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    )\n",
        "}\n",
        "\n",
        "# Define a method for displaying Earth Engine image tiles on a folium map.\n",
        "def add_ee_layer(self, ee_object, vis_params, name):\n",
        "    \n",
        "    try:    \n",
        "        # display ee.Image()\n",
        "        if isinstance(ee_object, ee.image.Image):    \n",
        "            map_id_dict = ee.Image(ee_object).getMapId(vis_params)\n",
        "            folium.raster_layers.TileLayer(\n",
        "            tiles = map_id_dict['tile_fetcher'].url_format,\n",
        "            attr = 'Google Earth Engine',\n",
        "            name = name,\n",
        "            overlay = True,\n",
        "            control = True\n",
        "            ).add_to(self)\n",
        "        # display ee.ImageCollection()\n",
        "        elif isinstance(ee_object, ee.imagecollection.ImageCollection):    \n",
        "            ee_object_new = ee_object.mosaic()\n",
        "            map_id_dict = ee.Image(ee_object_new).getMapId(vis_params)\n",
        "            folium.raster_layers.TileLayer(\n",
        "            tiles = map_id_dict['tile_fetcher'].url_format,\n",
        "            attr = 'Google Earth Engine',\n",
        "            name = name,\n",
        "            overlay = True,\n",
        "            control = True\n",
        "            ).add_to(self)\n",
        "        # display ee.Geometry()\n",
        "        elif isinstance(ee_object, ee.geometry.Geometry):    \n",
        "            folium.GeoJson(\n",
        "            data = ee_object.getInfo(),\n",
        "            name = name,\n",
        "            overlay = True,\n",
        "            control = True\n",
        "        ).add_to(self)\n",
        "        # display ee.FeatureCollection()\n",
        "        elif isinstance(ee_object, ee.featurecollection.FeatureCollection):  \n",
        "            ee_object_new = ee.Image().paint(ee_object, 0, 2)\n",
        "            map_id_dict = ee.Image(ee_object_new).getMapId(vis_params)\n",
        "            folium.raster_layers.TileLayer(\n",
        "            tiles = map_id_dict['tile_fetcher'].url_format,\n",
        "            attr = 'Google Earth Engine',\n",
        "            name = name,\n",
        "            overlay = True,\n",
        "            control = True\n",
        "        ).add_to(self)\n",
        "    \n",
        "    except:\n",
        "        print(\"Could not display {}\".format(name))\n",
        "    \n",
        "# Add EE drawing method to folium.\n",
        "folium.Map.add_ee_layer = add_ee_layer\n",
        "\n",
        "print(\"Packages installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C4zmuTIA8SR"
      },
      "source": [
        "## **1.2 Imported variables**\n",
        "These are Landsat-specific variable settings for GEE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CkiZcezXBEG5"
      },
      "outputs": [],
      "source": [
        "#@markdown __Run this block__ to import pre-existing features, images, and collections from Earth Engine\n",
        "wrs2 = ee.FeatureCollection(\"users/christinaherrickunh/WRS2_descending_2018\")\n",
        "utmbounds = ee.FeatureCollection(\"users/christinaherrickunh/UTM_Zone_Boundaries\")\n",
        "sw = ee.Image(\"JRC/GSW1_4/GlobalSurfaceWater\")\n",
        "openwater = sw.select(\"occurrence\")\n",
        "l4t1 = ee.ImageCollection(\"LANDSAT/LT04/C02/T1_L2\")\n",
        "l4t2 = ee.ImageCollection(\"LANDSAT/LT04/C02/T2_L2\")\n",
        "l5t1 = ee.ImageCollection(\"LANDSAT/LT05/C02/T1_L2\")\n",
        "l5t2 = ee.ImageCollection(\"LANDSAT/LT05/C02/T2_L2\")\n",
        "l7t1 = ee.ImageCollection(\"LANDSAT/LE07/C02/T1_L2\")\n",
        "l7t2 = ee.ImageCollection(\"LANDSAT/LE07/C02/T2_L2\")\n",
        "l8t1 = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\")\n",
        "l8t2 = ee.ImageCollection(\"LANDSAT/LC08/C02/T2_L2\")\n",
        "\n",
        "\n",
        "def prep457bands(img):\n",
        "  systime = img.get('system:time_start')\n",
        "  elev = img.get(\"SUN_ELEVATION\")\n",
        "  sza = ee.Number(90).subtract(elev)\n",
        "  \n",
        "  qa = img.select([\"QA_PIXEL\"])\n",
        "  cloud1 = qa.bitwiseAnd(2).eq(0)  # bit 1, dilated cloud\n",
        "  cloud3 = qa.bitwiseAnd(8).eq(0)  # bit 3, cloud\n",
        "  cloudshadow = qa.bitwiseAnd(16).eq(0) # bit 4, cloud shadow\n",
        "  snow = qa.bitwiseAnd(32).eq(0)  # bit 5, snow\n",
        "  cloud_confid = qa.rightShift(8).bitwiseAnd(3).lt(2)  # bits 8-9\n",
        "  cloudsh_confid = qa.rightShift(10).bitwiseAnd(3).lt(2)  # bits 10-11\n",
        "  snow_confid = qa.rightShift(12).bitwiseAnd(3).lt(2)  # bits 12-13\n",
        "  cirrus_confid = qa.rightShift(14).bitwiseAnd(3).lt(2)  # bits 14-15\n",
        "\n",
        "  temp = img.select([\"ST_B6\"],[\"surface_temp\"]).multiply(0.00341802).add(149).add(-273.15)\n",
        "  \n",
        "  updated = (temp.updateMask(cloud1)\n",
        "  .updateMask(cloud3)\n",
        "  .updateMask(snow)\n",
        "  .updateMask(cloudshadow)\n",
        "  .updateMask(cloud_confid)\n",
        "  .updateMask(snow_confid)\n",
        "  .updateMask(cirrus_confid)\n",
        "  .updateMask(cloudsh_confid))\n",
        "          \n",
        "  return ee.Image(updated).copyProperties(img).set(\"system:time_start\",systime).set(\"SOLAR_ZENITH_ANGLE\", sza)\n",
        "\n",
        "def prep8bands(img):\n",
        "  systime = img.get('system:time_start')\n",
        "  elev = img.get(\"SUN_ELEVATION\")\n",
        "  sza = ee.Number(90).subtract(elev)\n",
        "\n",
        "  qa = img.select([\"QA_PIXEL\"])\n",
        "  cloud1 = qa.bitwiseAnd(2).eq(0)  # bit 1, dilated cloud\n",
        "  cloud2 = qa.bitwiseAnd(4).eq(0)  # bit 2, cirrus (high confidence)\n",
        "  cloud3 = qa.bitwiseAnd(8).eq(0)  # bit 3, cloud\n",
        "  cloudshadow = qa.bitwiseAnd(16).eq(0) # bit 4, cloud shadow\n",
        "  snow = qa.bitwiseAnd(32).eq(0)  # bit 5, snow\n",
        "  cloud_confid = qa.rightShift(8).bitwiseAnd(3).lt(2)  # bits 8-9\n",
        "  cloudsh_confid = qa.rightShift(10).bitwiseAnd(3).lt(2)  # bits 10-11\n",
        "  snow_confid = qa.rightShift(12).bitwiseAnd(3).lt(2)  # bits 12-13\n",
        "  cirrus_confid = qa.rightShift(14).bitwiseAnd(3).lt(2)  # bits 14-15\n",
        "  \n",
        "  temp = img.select([\"ST_B10\"],[\"surface_temp\"]).multiply(0.00341802).add(149).add(-273.15)\n",
        "\n",
        "  updated = (temp.updateMask(cloud1)\n",
        "    .updateMask(cloud2)\n",
        "    .updateMask(cloud3)\n",
        "    .updateMask(cloudshadow)\n",
        "    .updateMask(snow)\n",
        "    .updateMask(cloud_confid)\n",
        "    .updateMask(cloudsh_confid)\n",
        "    .updateMask(snow_confid)\n",
        "    .updateMask(cirrus_confid))\n",
        "\n",
        "  return ee.Image(updated).copyProperties(img).set(\"system:time_start\",systime).set(\"SOLAR_ZENITH_ANGLE\", sza)\n",
        "\n",
        "\n",
        "print(\"variables and functions imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxgpEciUBKLy"
      },
      "source": [
        "## **1.3 Define the lake area**\n",
        "\n",
        "In this section, use either *Option A*, where you define the bounding box, or *Option B* where you import a shape file of your lake. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JiuNetjpgpJ"
      },
      "source": [
        "###Option A: use a bounding box\n",
        "Use the Rectangle tool on the map to draw a bounding box. The bounding box should be as small as possible while still including the entire lake surface. It cannot be a centroid location.<p>Do not run this block if you are using Option B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P7-dEgc0A9_0"
      },
      "outputs": [],
      "source": [
        "#@markdown Run this block to generate a map and use the Rectangle tool to draw a bounding box. \n",
        "#@markdown The bounding box should be as small as possible while still including the entire lake surface. It cannot be a centroid location.\n",
        "#@markdown If more than one box is drawn, the most recently drawn box will be used.\n",
        "#@markdown <p>Do not run this block if you are using Option B.\n",
        "\n",
        "from ipyleaflet import SearchControl, Marker, AwesomeIcon\n",
        "\n",
        "'''pure leaflet'''\n",
        "# baseMap = basemap_to_tiles(basemaps.CartoDB.DarkMatter)\n",
        "m = Map(center=(15,0), zoom=2, scroll_wheel_zoom=True, layout=Layout(width='80%', height='500px'))\n",
        "\n",
        "draw_control = DrawControl(rectangle={\"shapeOptions\":{\"fillColor\":\"#3fca45d\", \"color\":\"#efed69\", \"fillOpacity\": 0.25}}, circle={}, circlemarker={}, polygon={}, polyline={})\n",
        "search_control = SearchControl(position=\"topright\", url='https://nominatim.openstreetmap.org/search?format=json&q={s}', zoom=7)\n",
        "\n",
        "gj = []\n",
        "def handle_draw(self, action, geo_json):\n",
        "  if action in ['created','edited']:\n",
        "    gj.append(geo_json)\n",
        "    box = ee.Geometry.Polygon(gj[-1][\"geometry\"][\"coordinates\"],None, False, 100)\n",
        "    rec_geojson = gj[-1][\"geometry\"][\"coordinates\"]\n",
        "    lon1, lat1 = rec_geojson[0][0]\n",
        "    lon2, lat2 = rec_geojson[0][2]\n",
        "\n",
        "draw_control.on_draw(handle_draw)\n",
        "m.add_control(draw_control)\n",
        "m.add_control(search_control)\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CX2G1Rtn_RNe"
      },
      "outputs": [],
      "source": [
        "#@markdown Run this block to set up your bounding box coordinates.\n",
        "use_user_input_file = \"no\"\n",
        "try:\n",
        "  box = ee.Geometry.Polygon(gj[-1][\"geometry\"][\"coordinates\"],None, False, 100)\n",
        "  rec_geojson = gj[-1][\"geometry\"][\"coordinates\"]\n",
        "  lon1, lat1 = rec_geojson[0][0]\n",
        "  lon2, lat2 = rec_geojson[0][2]\n",
        "\n",
        "  center_lat = (lat1+lat2)/2\n",
        "  center_lon = (lon1+lon2)/2\n",
        "\n",
        "  print(f'''\n",
        "  north bounds: {lat1}\n",
        "  south bounds: {lat2}\n",
        "  west bounds: {lon1}\n",
        "  east bounds: {lon2}\n",
        "  ''')\n",
        "except IndexError:\n",
        "  print(\"An error occurred, usually because this block was run but no box was drawn on the above map\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBZzaGgqpr8W"
      },
      "source": [
        "###Option B: upload a table file\n",
        "Using your Google Earth Engine account, upload a shapefile or csv of your study lake to your Assets and link to it here.<p>Click [here](https://drive.google.com/file/d/1lfFoZzQD_wAA7Bnil7mI4zwjKDG5kegh/view?usp=sharing) and [here](https://drive.google.com/file/d/1KNaZV63J_LUp6X1OcoLX1wonF0ASiIzi/view?usp=sharing) for screenshots of the process. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e0lONKBvqAUp"
      },
      "outputs": [],
      "source": [
        "#@markdown Copy the asset path from Google Earth Engine and change\n",
        "#@markdown dropdown to `yes`.\n",
        "#@markdown File path should begin with 'users/'\n",
        "user_input_file = \"\" #@param {type: \"string\"}\n",
        "use_user_input_file = \"no\" #@param [\"yes\",\"no\"] {allow-input: false}\n",
        "#@markdown Remember to __run this block__ after filling in the above fields.\n",
        "#@markdown Do not run this block if you are using Option A.\n",
        "\n",
        "if use_user_input_file==\"no\":\n",
        "  print(\"If you intend to use Option B, change the 'use_user_input_file' variable to 'yes' and rerun\")\n",
        "elif len(user_input_file)>1:\n",
        "  shp = ee.FeatureCollection(user_input_file).geometry()\n",
        "  centroid = shp.centroid(10).getInfo()[\"coordinates\"]\n",
        "  center_lat = centroid[1]\n",
        "  center_lon = centroid[0]\n",
        "  print(center_lat)\n",
        "  print(center_lon)\n",
        "else:\n",
        "  print(\"No file input; will use Opt A bounding box coordinates\")\n",
        "  use_user_input_file = \"no\"\n",
        "\n",
        "if 'shp' in globals() or 'shp' in locals():\n",
        "  shp_map = folium.Map(location=[center_lat,center_lon],zoom_start=10,width=600,height=600)\n",
        "  shp_map.add_child(folium.LatLngPopup())\n",
        "  basemaps['Google Maps'].add_to(shp_map)\n",
        "  shp_map.add_ee_layer(shp,{},'aoi')\n",
        "  display(shp_map)\n",
        "  box = shp\n",
        "# else:\n",
        "#   print(\"Display does not apply here; Opt A bounding box is being used for AOI\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDrkGq_jwgIo"
      },
      "source": [
        "## **1.4 Set remaining user variables**\n",
        "\n",
        "This section defines acceptable water persistence and error for the Landsat pixels, the time period of interest, and confirms the path and row for Landsat acquisition. You can also change the default Google Drive folder for any outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "L5s5K2jQm9Zt"
      },
      "outputs": [],
      "source": [
        "#@markdown We suggest saving the lakeCoSTR data to it's own folder in your Google Drive. \n",
        "#@markdown Replace your lake name where the following prompts read 'LAKENAME'. These folders\n",
        "#@markdown will be automatically created within this script.\n",
        "\n",
        "lake_name = 'LAKENAME' #@param {type:\"string\"}\n",
        "output_dir = \"/content/drive/MyDrive/lakeCoSTR_output/\"+ lake_name\n",
        "\n",
        "#this parses the 'output_dir' string to get last listed subfolder\n",
        "short_dir = lake_name\n",
        "file_prefix = lake_name\n",
        "print(f\"Your data will be saved in {output_dir}\")\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "def check_splcharacter(test):\n",
        "  string_check = re.compile('[@!#$%^&*() <>?/\\|}{~:]')\n",
        "  if not string_check.search(test)==None:\n",
        "    print(\"Your file prefix contains special characters, please fix\")\n",
        "    lake_name = \"LAKENAME\"\n",
        "  else:\n",
        "    print(\"Files will all have the prefix:\",test)\n",
        "\n",
        "check_splcharacter(file_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RwhZWCrkVrb0"
      },
      "outputs": [],
      "source": [
        "#@markdown Boundaries of water bodies are automatically detected using the JRC \n",
        "#@markdown Global Surface Water Mapping Layer dataset of percent water occurrence. \n",
        "#@markdown By default, a pixel has to be classified as water at least 55% of the \n",
        "#@markdown time. This is defined as `pctTime`. If your lake of interest experiences \n",
        "#@markdown significant fluctuations of lake level, we suggest using a higher percentage than the default. \n",
        "\n",
        "#@markdown <p>More information on this dataset can be found at https://doi.org/10.1038/nature20584\n",
        "\n",
        "\n",
        "\n",
        "pctTime = 55  #@param {type: \"number\"}\n",
        "#@markdown __Run this block__ after inputting the above parameter\n",
        "\n",
        "water_fig = folium.Figure(width=\"60%\")\n",
        "water_map = folium.Map(location=[center_lat,center_lon],zoom_start=10,tiles=None).add_to(water_fig)\n",
        "water_map.add_child(folium.LatLngPopup())\n",
        "\n",
        "# blues = [\"f7fbff\",\"deebf7\",\"c6dbef\",\"9ecae1\",\"6baed6\",\"4292c6\",\"2171b5\",\"08519c\",\"08306b\"]\n",
        "blues = [\"FFFFFF\",\"08306b\"]\n",
        "\n",
        "# basemaps['Google Satellite'].add_to(water_map)\n",
        "folium.TileLayer('cartodbdark_matter').add_to(water_map)\n",
        "water_map.add_ee_layer(box,{},'aoi')\n",
        "water_map.add_ee_layer(openwater,{\"min\":pctTime, \"max\":100, \"palette\":blues}, 'water presence')\n",
        "# water_map.add_ee_layer(box,{},'aoi')\n",
        "print(\"Occurrence of water detected by Landsat where water is present at least \",pctTime,\"% of the time (in blue)\")\n",
        "print(\"Water occurrence less than \", pctTime, \"% of the time are in white\")\n",
        "display(water_fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N9WlKk7iHcRC"
      },
      "outputs": [],
      "source": [
        "#@markdown Landsat scenes from both Tier 1 and Tier 2 of Collection 2 are \n",
        "#@markdown included for possible use. For Tier 2, specify the maximum spatial \n",
        "#@markdown error (in meters) for pixels. Only orthorectified images (L1T) are \n",
        "#@markdown considered. The default value is 24 meters.\n",
        "rmse = 24  #@param {type: \"number\"}\n",
        "#@markdown __Run this block__ after inputting the above parameter\n",
        "print(f\"Tier 2 L1T scenes with a maximum root mean square error of {rmse} meters will be considered\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ypJaiep9n1F7"
      },
      "outputs": [],
      "source": [
        "#@markdown Landsat thermal band records begin in 1982 with Landsat 4 Thematic Mapper (TM). \n",
        "#@markdown Enter the year range that you want to search, as well as months of \n",
        "#@markdown the year. To search all months, `m1 = 1` and `m2 = 12` <p>\n",
        "#@markdown <p>First year (y1) and last year (y2). Years are inclusive, minimum value is 1982. <p>\n",
        "#@markdown <p>First month (m1) and last month (m2). Months are inclusive.<p>\n",
        "#@markdown <p>Note that this method has only been tested for temperature estimation where there is no ice present on the lake. \n",
        "\n",
        "y1 = 1982 #@param {type: \"number\"}\n",
        "y2 =  2020#@param {type: \"number\"}\n",
        "\n",
        "m1 =  5#@param {type: \"number\"}\n",
        "m2 = 11 #@param {type: \"number\"}\n",
        "print('Script will search years %d through %d and months %d through %d of each year' % (y1,y2,m1,m2))\n",
        "#@markdown __Run this block__ after inputting the above parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xE7MPgvY78u"
      },
      "source": [
        "# **2. Find water pixels & delineate boundary**\n",
        "This step uses the bounding box or the table file as well as the water persistence using [JRC Water dataset](https://global-surface-water.appspot.com/) to define the waterbody area. \n",
        "\n",
        "Note, in some cases the _Opt A_ bounding box method will include upstream and downstream water areas, particularily where there are rivers that are large enough to be detected by Landsat. To avoid this, use the _Opt B_ table file method. <p>\n",
        "You can explore the JRC Water dataset [here](https://global-surface-water.appspot.com/).<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V1ljgJffQF1Y"
      },
      "outputs": [],
      "source": [
        "#@markdown This code block reduces the bounding box defined above to the largest water body present, and then counts the 30m pixels within the water body.<p>\n",
        "#@markdown Landsat scenes included in analysis must have a minimum percentage of clear lake pixels available (these are discontinuous pixels). Default is 25%.\n",
        "pctLakeCoverage = 25 #@param {type: \"number\"}\n",
        "#@markdown __Run this block__ after inputting the above parameter\n",
        "\n",
        "if use_user_input_file==\"yes\":\n",
        "  try:\n",
        "    box = shp\n",
        "  except NameError:\n",
        "    raise NameError('''Cannot find user-input file. Are you using Option B or \n",
        "    Opt A bounding box? You may need to correct and rerun.''')\n",
        "\n",
        "wateroccurrence = sw.select(0)\n",
        "water = wateroccurrence.gte(pctTime)\n",
        "water = water.updateMask(water.neq(0))\n",
        " \n",
        "\n",
        "#########################\n",
        "# Find the biggest waterbody in the bounding box\n",
        "def addArea(feature):\n",
        "  # returns area +/- 1sqm\n",
        "  return feature.set({'area':feature.geometry().area(1)})  \n",
        "\n",
        "regions = water.addBands(wateroccurrence).reduceToVectors(\n",
        "    reducer=ee.Reducer.min(),\n",
        "    geometry=box,\n",
        "    scale=30,\n",
        "    maxPixels=5e9,\n",
        "    geometryInNativeProjection=False,\n",
        "    labelProperty='surfaceWater').map(addArea).sort('area',False);\n",
        "\n",
        "lake_outline = ee.Feature(regions.first())\n",
        "geo = lake_outline.geometry()\n",
        "\n",
        "try:\n",
        "  #########################\n",
        "  # Find the best utm zone over the lake to generate CRS\n",
        "\n",
        "  utm = utmbounds.filterBounds(box)\n",
        "  utmJoin = ee.Join.saveBest('matches','best');\n",
        "  utmCondition = ee.Filter.intersects(**{\n",
        "      \"rightField\": '.geo',\n",
        "      \"leftField\": '.geo'\n",
        "  })\n",
        "  utmJoined = utmJoin.apply(lake_outline, utm, utmCondition)\n",
        "  \n",
        "  try:\n",
        "    getZone = utmJoined.getInfo()[\"features\"][0][\"properties\"][\"matches\"][\"properties\"][\"ZONE\"]\n",
        "    getHemi = utmJoined.getInfo()[\"features\"][0][\"properties\"][\"matches\"][\"properties\"][\"HEMISPHERE\"]\n",
        "\n",
        "    #indicate hemisphere  \n",
        "    if getHemi == 'n':\n",
        "      crs_out = f\"EPSG:326{getZone}\"\n",
        "    else:\n",
        "      crs_out = f\"EPSG:327{getZone}\"\n",
        "    print(\"Data will be calculated with output CRS:\",crs_out)\n",
        "    #########################\n",
        "    # print the user some pixel count statistics\n",
        "\n",
        "    watercount = water.setDefaultProjection(crs_out).reduceRegion(reducer=ee.Reducer.count().unweighted(), \n",
        "                                    geometry=lake_outline.geometry(),\n",
        "                                    scale=30, bestEffort=True)\n",
        "    lakesurface = watercount.getInfo()[\"occurrence\"]  # total pixels over lake\n",
        "\n",
        "    pixel_min = round(lakesurface*(pctLakeCoverage/100.0))\n",
        "    print(\"\\nTotal # of lake pixels: \", lakesurface);\n",
        "    print(f\"{pctLakeCoverage}% of available lake pixels is\", pixel_min)\n",
        "\n",
        "    if lakesurface < 42:\n",
        "      print('''Because lakeCoSTR only supplies aggregated data, \n",
        "      the lake you have selected has too few pixels available for \n",
        "      meaningful data aggregation. \n",
        "      It is not recommended that you use lakeCoSTR for this lake.''')\n",
        "    else:\n",
        "      #########################\n",
        "      # Find the overlapping landsat path/rows\n",
        "\n",
        "      pathrow = wrs2.filterBounds(geo)\n",
        "        \n",
        "      num_of_pr = len(pathrow.getInfo()[\"features\"])\n",
        "      path_west = 1\n",
        "      path_east = 233\n",
        "      row_north = 122\n",
        "      row_south = 1\n",
        "\n",
        "      for i in range(0,num_of_pr):\n",
        "        pr = pathrow.getInfo()[\"features\"][i][\"properties\"][\"PR\"]\n",
        "        p = pr[:3]\n",
        "        r = pr[3:]\n",
        "        if int(p) > path_west:\n",
        "          path_west = int(p)\n",
        "        if int(p) < path_east:\n",
        "          path_east = int(p)\n",
        "        if int(r) < row_north:\n",
        "          row_north = int(r)\n",
        "        if int(r) > row_south:\n",
        "          row_south = int(r)\n",
        "\n",
        "      p1 = path_west\n",
        "      p2 =  path_east\n",
        "      r1 =  row_north\n",
        "      r2 =  row_south\n",
        "      print(f\"\\nYour lake overlaps {num_of_pr} landsat footprint(s)\")\n",
        "      print(f'bounding path(s): {path_west} (west) to {path_east} (east)')\n",
        "      print(f'bounding row(s): {row_north} (north) to {row_south} (south)\\n')\n",
        "\n",
        "\n",
        "      ##### Attempt to find the \"best\" path/row by comparing footprints that completely\n",
        "      ## contain the lake, or if none completely contain, find the footprint that has\n",
        "      ## the most overlap\n",
        "\n",
        "      footprintJoinAll = ee.Join.saveAll('matches')\n",
        "      footprintJoinBest = ee.Join.saveBest(**{'matchKey':'matches','measureKey':'best'})\n",
        "\n",
        "      footprintContains = ee.Filter.isContained(**{'rightField':'.geo','leftField':'.geo'})\n",
        "      footprintIntersects = ee.Filter.intersects(**{'rightField':'.geo','leftField':'.geo'})\n",
        "\n",
        "      allContained = footprintJoinAll.apply(lake_outline,pathrow,footprintContains)\n",
        "      anyIntersects = footprintJoinBest.apply(lake_outline,pathrow,footprintIntersects)\n",
        "\n",
        "      try:\n",
        "        containedMatches = allContained.getInfo()[\"features\"][0][\"properties\"][\"matches\"]\n",
        "      except IndexError:\n",
        "        containedMatches = []\n",
        "\n",
        "      intersectedMatches = anyIntersects.getInfo()[\"features\"][0][\"properties\"][\"matches\"]\n",
        "\n",
        "      #########################\n",
        "      # Display a map\n",
        "\n",
        "      pixel_fig = folium.Figure(width=\"50%\")\n",
        "      pixel_map = folium.Map(location=[center_lat,center_lon],zoom_start=6).add_to(pixel_fig)\n",
        "      basemaps['Google Maps'].add_to(pixel_map)\n",
        "      pixel_map.add_ee_layer(pathrow,{},'path row')\n",
        "\n",
        "      def addFPtoMap(list_of_fps):\n",
        "        for fp in list_of_fps:\n",
        "          makeGeojson = folium.GeoJson(fp[\"geometry\"])\n",
        "          fpPath = str(fp[\"properties\"][\"PATH\"])\n",
        "          fpRow = str(fp[\"properties\"][\"ROW\"])\n",
        "          popup = folium.Popup(\"P\"+fpPath+\" R\"+fpRow)\n",
        "          popup.add_to(makeGeojson)\n",
        "          makeGeojson.add_to(pixel_map)\n",
        "\n",
        "      if len(containedMatches)>0: #lake is fully contained by at least 1 path-row\n",
        "        if num_of_pr == 1: \n",
        "          addFPtoMap(containedMatches)\n",
        "          print('''Great news! Your lake is completely contained by a single path-row footprint.\n",
        "          Because your lake does not cross multiple WRS2 rows, there's nothing more you need to do.\n",
        "          You can skip the next codeblock and move to the 'Get Skin Surface Temperatures' section.\n",
        "          ''')\n",
        "        elif len(intersectedMatches) == 0: #and the lake doesn't have any partial matches\n",
        "          addFPtoMap(containedMatches)\n",
        "          if(row_north-row_south) == 0: #if it is contained by only one row\n",
        "            print('''Great news! Your lake is completely contained by at least one path-row footprint.\n",
        "            Because your lake does not cross multiple WRS2 rows, there's nothing more you need to do.\n",
        "            You can skip the next codeblock and move to the 'Get Skin Surface Temperatures' section.\n",
        "            ''')\n",
        "          else: #if it is contained by multiple rows\n",
        "            print('''Great news! Your lake is completely contained by multiple WRS2 path-row footprints, \n",
        "            HOWEVER, those footprints include multiple WRS2 rows in the same WRS2 path. In order to not retreive near-duplicate \n",
        "            temperature estimates from neghboring WRS2 rows, please *manually input* a single ROW value for `new_row_north` and \n",
        "            `new_row_south` in the codeblock below. We reccommend you choose the row that is closest to the equator (a lower row number in the Northern \n",
        "            Hemisphere, or a higher row number in the Southern Hemisphere).\n",
        "            ''')\n",
        "            print(\"Click the blue highlighted footprint to see the path and row\")\n",
        "        elif len(intersectedMatches)>0: #if it has partial matches\n",
        "          if len(containedMatches) >0: #and it has fully contained matches\n",
        "            addFPtoMap(containedMatches)\n",
        "            print('''Great news! Your lake is completely contained by at least one path-row footprint, \n",
        "            HOWEVER, you need to *manually input* the path and row values for the blue box(es) displayed \n",
        "            in the map below into the next code block because the overlapping path-row combinations \n",
        "            outlined in black do/does not completely contain your lake.\n",
        "            ''')\n",
        "            print(\"Click the blue highlighted footprint to see the path and row\")\n",
        "      else: #otherwise, you're outta luck\n",
        "        print('''\n",
        "        CAUTION: Your lake is not completely contained by a single WRS2 path-row footprint. lakeCoSTR functions best\n",
        "        when your lake is completely contained by at least one WRS2 path-row footprint. Because of this, we do not \n",
        "        recommend the use of lakeCoSTR to acquire surface temperature estimates for your lake. \n",
        "        ''')\n",
        "\n",
        "      pixel_map.add_ee_layer(lake_outline.geometry(), {}, 'lake area')\n",
        "      display(pixel_map)\n",
        "  except:\n",
        "    print('''An error occurred - either you have different selections of lake area and \n",
        "    water occurrance area (double check sections 1.3 and 1.4) or the lake you selected is \n",
        "    too large to process in lakeCoSTR.''')\n",
        "except IndexError:\n",
        "  print('''An error occurred - this could be because there are no water pixels available for analysis.\n",
        "  Check the map in above step to see if there are water pixels available for analysis.''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uefURvxRYjlS"
      },
      "outputs": [],
      "source": [
        "#@markdown If ther previous code block directed you to `*manually input*` WRS2 path-row values, \n",
        "#@markdown please enter the values for path ('P') and row ('R') as indicated by clicking on the\n",
        "#@markdown blue box in the map above. \n",
        "\n",
        "#@markdown <br>If you weren't directed to change your the WRS2 path-row parameters, leave these blank.\n",
        "\n",
        "new_path_west = \"\" #@param {type:'string'}\n",
        "new_path_east = \"\" #@param {type: 'string'}\n",
        "new_row_north = \"\" #@param {type: 'string'}\n",
        "new_row_south = \"\" #@param {type: 'string'}\n",
        "\n",
        "if new_path_west:\n",
        "  p1 = int(new_path_west)\n",
        "else: \n",
        "  p1 =  path_west\n",
        "if new_path_east:\n",
        "  p2 =  int(new_path_east)\n",
        "else:\n",
        "  p2 = path_east\n",
        "if new_row_north:\n",
        "  r1 =  int(new_row_north)\n",
        "else:\n",
        "  r1 = row_north\n",
        "if new_row_south:\n",
        "  r2 =  int(new_row_south)\n",
        "else:\n",
        "  r2 = row_south\n",
        "\n",
        "print(f'new bounding path(s): {p1} (west) to {p2} (east)')\n",
        "print(f'new bounding row(s): {r1} (north) to {r2} (south)\\n')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45tElCUuotvD"
      },
      "source": [
        "# **3. Get Skin Surface Temperatures**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1TjyRw18ttpV"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@markdown **Run this block** to filter by date and site location, convert \n",
        "#@markdown temperature from K --> C, compile and stack images, and carry over the metadata. \n",
        "\n",
        "#For landsat 8, any scenes with possible image quality \n",
        "#issues due to Scene Select Mirror (SSM) position are removed.\n",
        "\n",
        "l4 = (l4t1.merge(l4t2).filterMetadata('L1_PROCESSING_LEVEL','equals','L1TP') \\\n",
        "      .filterMetadata('GEOMETRIC_RMSE_MODEL','not_greater_than',rmse) \\\n",
        "      .filterBounds(geo) \\\n",
        "      .filter(ee.Filter.calendarRange(y1,y2,'year')) \\\n",
        "      .filter(ee.Filter.calendarRange(m1,m2,'month')) \\\n",
        "      .filterMetadata('WRS_ROW','not_less_than',r1).filterMetadata('WRS_ROW','not_greater_than',r2) \\\n",
        "      .filterMetadata('WRS_PATH','not_greater_than',p1).filterMetadata('WRS_PATH','not_less_than',p2) \\\n",
        "      .map(prep457bands, True))\n",
        "\n",
        "l5 = (l5t1.merge(l5t2).filterMetadata('L1_PROCESSING_LEVEL','equals','L1TP') \\\n",
        "      .filterMetadata('GEOMETRIC_RMSE_MODEL','not_greater_than',rmse) \\\n",
        "      .filterBounds(geo) \\\n",
        "      .filter(ee.Filter.calendarRange(y1,y2,'year')) \\\n",
        "      .filter(ee.Filter.calendarRange(m1,m2,'month')) \\\n",
        "      .filterMetadata('WRS_ROW','not_less_than',r1).filterMetadata('WRS_ROW','not_greater_than',r2) \\\n",
        "      .filterMetadata('WRS_PATH','not_greater_than',p1).filterMetadata('WRS_PATH','not_less_than',p2) \\\n",
        "      .map(prep457bands, True))\n",
        "\n",
        "l7 = (l7t1.merge(l7t2).filterMetadata('L1_PROCESSING_LEVEL','equals','L1TP') \\\n",
        "      .filterMetadata('GEOMETRIC_RMSE_MODEL','not_greater_than',rmse) \\\n",
        "      .filterBounds(geo) \\\n",
        "      .filter(ee.Filter.calendarRange(y1,y2,'year')) \\\n",
        "      .filter(ee.Filter.calendarRange(m1,m2,'month')) \\\n",
        "      .filterMetadata('WRS_ROW','not_less_than',r1).filterMetadata('WRS_ROW','not_greater_than',r2) \\\n",
        "      .filterMetadata('WRS_PATH','not_greater_than',p1).filterMetadata('WRS_PATH','not_less_than',p2) \\\n",
        "      .map(prep457bands, True))\n",
        "\n",
        "l8 = (l8t1.merge(l8t2).filterMetadata('L1_PROCESSING_LEVEL','equals','L1TP') \\\n",
        "      .filterMetadata('GEOMETRIC_RMSE_MODEL','not_greater_than',rmse) \\\n",
        "      .filterBounds(geo) \\\n",
        "      .filter(ee.Filter.calendarRange(y1,y2,'year')) \\\n",
        "      .filter(ee.Filter.calendarRange(m1,m2,'month')) \\\n",
        "      .filterMetadata('WRS_ROW','not_less_than',r1).filterMetadata('WRS_ROW','not_greater_than',r2) \\\n",
        "      .filterMetadata('WRS_PATH','not_greater_than',p1).filterMetadata('WRS_PATH','not_less_than',p2) \\\n",
        "      .filterMetadata('TIRS_SSM_POSITION_STATUS','not_equals','SWITCHED') \\\n",
        "      .map(prep8bands, True))\n",
        "\n",
        "landsat = ee.ImageCollection(l4).merge(l5).merge(l7).merge(l8)\n",
        "\n",
        "print(\"Landsat compiled at\", strftime(\"%x %X\"))\n",
        "\n",
        "'''\n",
        "Add metadata to each scene that indicates how many visible pixels were included \n",
        "in analysis, and filter images so that only scenes with the minimum number of \n",
        "pixels remain.\n",
        "'''\n",
        "def countPixels(img):\n",
        "  img = ee.Image(img)\n",
        "  getCount = img.reduceRegion(**{\n",
        "      \"reducer\": ee.Reducer.count(),\n",
        "      \"geometry\": geo, \n",
        "      \"scale\": 30})\n",
        "  count = ee.Dictionary(getCount).get('surface_temp')\n",
        "  return img.set('pixel_count',count)\n",
        "before = landsat.map(countPixels)\n",
        "countedPixels = before.filterMetadata('pixel_count','not_less_than', pixel_min)\n",
        "print('Total number of LS scenes:',len(before.getInfo()['features']),'\\nNumber of LS scenes after filter:', len(countedPixels.getInfo()['features']))\n",
        "print(\"Landsat filtered:\", strftime(\"%x %X\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FleAQ6cyfch"
      },
      "source": [
        "# **4. Data Visualization**\n",
        "The code blocks in the following section allow you to explore your results using dataframes and pixel-value distributions. You can visually inspect each scene for errant data or export the data to explore further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "i8K6XN2xZMjP"
      },
      "outputs": [],
      "source": [
        "#@title 4.1 Convert data from server-side EE objects to client-side Python objects\n",
        "#@markdown __Run this block__ to define functions for data exploration\n",
        "\n",
        "\n",
        "# These functions convert image pixels to numpy arrays\n",
        "def createArrays(img):\n",
        "  psi_prop = ee.Image(img).sampleRectangle(region=geo, defaultValue=-1)\n",
        "  getProp = psi_prop.get(\"surface_temp\")\n",
        "  return img.set(\"img_array\", getProp)\n",
        "\n",
        "def getArrays(client_img_col):\n",
        "  list_of_arrays = []\n",
        "  for img in client_img_col:\n",
        "    prop = img[\"properties\"][\"img_array\"]\n",
        "    a = np.array(prop)\n",
        "    list_of_arrays.append(a)\n",
        "  return list_of_arrays\n",
        "print('Conversion function imported.')\n",
        "\n",
        "def generate_histogram(img):\n",
        "  img = ee.Image(img)\n",
        "  fhisto = img.reduceRegion(**{\n",
        "      \"reducer\": ee.Reducer.fixedHistogram(-5,30,70).unweighted(),\n",
        "      \"geometry\": geo,\n",
        "      \"scale\": 30,\n",
        "      \"maxPixels\": 5e9\n",
        "  })\n",
        "  return img.set('histogram',fhisto.get('surface_temp'))\n",
        "print(\"Will generate histograms with a fixed x-axis between -5 - 30 deg C in 0.5 degree bins\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "InafBTf5ZvAS"
      },
      "outputs": [],
      "source": [
        "# @markdown This code block executes the functions from the previous block and \n",
        "# @markdown converts the image collection from a server-side EE object to a client-side \n",
        "# @markdown Python object. This allows iteration over the image collection with a `for` \n",
        "# @markdown loop, which provides much more Python functionality.<p><mark>__This step \n",
        "# @markdown could take some time to run</mark> depending on the length of the \n",
        "# @markdown ImageCollection.__\n",
        "\n",
        "# @markdown If you recieve an error at this step that `your response size exceeds limit`, \n",
        "# @markdown you will need to go back and reduce the number of years of data you are requesting. \n",
        "\n",
        "print(\"Start:\", strftime(\"%x %X\"))\n",
        "imgs_histo = countedPixels.map(generate_histogram)\n",
        "generate_arrays = imgs_histo.map(createArrays)\n",
        "\n",
        "imgCol = generate_arrays.getInfo()[\"features\"]\n",
        "print(\"Finish:\", strftime(\"%x %X\"))\n",
        "print(\"Number of Landsat scenes: \", len(imgCol))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H-cobA_BbwRK"
      },
      "outputs": [],
      "source": [
        "#@markdown Now that the image collection is client-side, __run this block__ to \n",
        "#@markdown get arrays of each image.\n",
        "# returns a list of arrays\n",
        "imgs_to_arrays = getArrays(imgCol)\n",
        "print(\"Converted images to arrays\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "D9sJyjsi9eCF"
      },
      "outputs": [],
      "source": [
        "#@title 4.2 Collated pixel distributions\n",
        "#@markdown __Run this block__ to iterate through the image collection and convert \n",
        "#@markdown all bins and frequencies to a single Pandas `DataFrame`. \n",
        "\n",
        "length_plots = len(imgCol)\n",
        "list_of_dfs = []\n",
        "list_of_uids = []\n",
        "\n",
        "# fig = make_subplots(rows=length_plots, cols=1)\n",
        "for i in imgCol:\n",
        "  uid = (i['properties']['L1_LANDSAT_PRODUCT_ID'])\n",
        "  list_of_uids.append(uid)\n",
        "  histogram = i[\"properties\"].get(\"histogram\")\n",
        "\n",
        "  a = pd.DataFrame(histogram, columns=[\"bin\",uid]).set_index(\"bin\")\n",
        "  # fig.add_histogram()\n",
        "  list_of_dfs.append(a)\n",
        "\n",
        "appended_dfs = pd.concat(list_of_dfs,ignore_index=False, axis=1)\n",
        "transposed_dfs = appended_dfs.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2_AbqwwDcT9j"
      },
      "outputs": [],
      "source": [
        "#@title 4.3 View pixel distribution using `matplotlib`\n",
        "#@markdown __Run this block__ to generate histograms for each lake temperature \n",
        "#@markdown Landsat scene. Each scene is saved as png in a folder 'histograms' in the lakeCoSTR_output directory. \n",
        "#@markdown This step can take some time to run.\n",
        "\n",
        "#print(len(list_of_uids),\" histograms will be generated\")\n",
        "print(\"Start:\", strftime(\"%x %X\"))\n",
        "\n",
        "#make 'histogram' folder in defined directory\n",
        "if not os.path.exists(os.path.join(output_dir, 'histograms')):\n",
        "    os.makedirs(os.path.join(output_dir, 'histograms'))\n",
        "\n",
        "histo_dir = os.path.join(output_dir, 'histograms')\n",
        "\n",
        "print(\"Histograms will be saved at \", histo_dir)\n",
        "\n",
        "histos = appended_dfs\n",
        "#list of scenes\n",
        "scenes = histos.columns\n",
        "histos = pd.DataFrame.transpose(histos) #reorient data\n",
        "#reset index\n",
        "histos = histos.reset_index()\n",
        "histos_vert = histos.melt(id_vars=['index'])\n",
        "\n",
        "#iterate over the scene list\n",
        "for i in range(0, len(scenes)):\n",
        "  df = histos_vert[histos_vert['index'] == scenes[i]]\n",
        "  histoname = os.path.join(histo_dir,scenes[i]+'_histo.png')\n",
        "  plt.bar(x = df.bin, height = df.value)\n",
        "  plt.ylabel('frequency')\n",
        "  plt.xlabel('temperature (deg C)')\n",
        "  plt.title(scenes[i])\n",
        "  plt.savefig(histoname)\n",
        "  print('Saving histogram for Landsat scene ', scenes[i], ' at ', histoname)  \n",
        "  plt.close()\n",
        "\n",
        "print(\"Finish:\", strftime(\"%x %X\"), 'Your histograms should now be visible at ', histo_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1sYNsf_ndyG"
      },
      "source": [
        "# **5. Export Landsat temperature statistics to CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "t9STTRpLaRnD"
      },
      "outputs": [],
      "source": [
        "#@markdown Execute this cell block to export a CSV containing temperature and related image metadata to your Google Drive.\n",
        "\n",
        "def exportWholeLakeStats(img):\n",
        "  img = ee.Image(img).clip(geo)\n",
        "\n",
        "  # retrieve image metadata for output file\n",
        "  landsattime = img.get('system:time_start')\n",
        "  cloudcover_pct_scene = img.get('CLOUD_COVER')\n",
        "  esd_au_scene = img.get(\"EARTH_SUN_DISTANCE\")\n",
        "  sunelev_deg_scene = img.get('SUN_ELEVATION')\n",
        "  sunazi_deg_scene = img.get('SUN_AZIMUTH')\n",
        "  solzenang_deg_scene = img.get('SOLAR_ZENITH_ANGLE')\n",
        "  count = img.get('pixel_count')\n",
        "  pctAvail = ee.Number(count).divide(lakesurface).multiply(100)\n",
        "\n",
        "  # get statistics for skin_temperature\n",
        "  reducers = ee.Reducer.mean().combine(**{\n",
        "      \"reducer2\": ee.Reducer.stdDev(), \"sharedInputs\":True}).combine(**{\n",
        "      \"reducer2\": ee.Reducer.minMax(), \"sharedInputs\":True}).combine(**{\n",
        "      \"reducer2\": ee.Reducer.median(**{\"maxBuckets\":500, \"minBucketWidth\": 0.125}), \"sharedInputs\":True}).combine(**{\n",
        "      \"reducer2\": ee.Reducer.skew(), \"sharedInputs\":True}).combine(**{\n",
        "      \"reducer2\": ee.Reducer.kurtosis(), \"sharedInputs\":True}).combine(**{\n",
        "      \"reducer2\": ee.Reducer.percentile(**{\"percentiles\": [25,75], \"maxBuckets\": 500, \"minBucketWidth\": 0.125}), \"sharedInputs\":True})\n",
        "      \n",
        "  \n",
        "  stats = img.reduceRegion(**{\n",
        "      \"reducer\": reducers,\n",
        "      \"geometry\": geo,\n",
        "      \"scale\": 30,\n",
        "      \"crs\": crs_out,\n",
        "      \"maxPixels\": 5e9\n",
        "  })\n",
        "  \n",
        "  more_stats = ee.Dictionary({'availablePixels_count':count,\n",
        "                            'datetime_landsat':landsattime,\n",
        "                            'cloudcover_pct_scene':cloudcover_pct_scene,\n",
        "                            'sunelev_deg_scene':sunelev_deg_scene,\n",
        "                            'sunazi_deg_scene':sunazi_deg_scene,\n",
        "                            'esd_au_scene':esd_au_scene,\n",
        "                            'sunazi_deg_scene':sunazi_deg_scene,\n",
        "                            'lakeCoverage_pct':pctAvail,\n",
        "                            #'datetime_landsat_excelformat':ee.Number(landsattime).divide(1000.0).divide(86400).add(25569)\n",
        "                            })\n",
        "  stats2 = ee.Dictionary.combine(stats, more_stats)\n",
        "\n",
        "  return ee.Feature(None,stats2)\n",
        "\n",
        "temp_stats = countedPixels.map(exportWholeLakeStats)\n",
        "\n",
        "#create the file name with user-specified prefix\n",
        "temp_filename = (file_prefix+'_temp_stats')\n",
        "\n",
        "export_task = ee.batch.Export.table.toDrive(**{\n",
        "    'collection': temp_stats,\n",
        "    'description': temp_filename,\n",
        "    \"fileFormat\": \"CSV\",\n",
        "    'folder': short_dir\n",
        "})\n",
        "\n",
        "print(\"Exporting \", temp_filename+\".csv\")\n",
        "export_task.start()\n",
        "print('Polling for task (id: {}) at'.format(export_task.id))\n",
        "\n",
        "while export_task.active():\n",
        "  print(strftime(\"%x %X\"), export_task.status())\n",
        "  sleep(10)\n",
        "\n",
        "print(\"Finished:\", strftime(\"%x %X\"))\n",
        "\n",
        "print('Export should now be visible in Drive at path:\\n',os.path.join(output_dir, temp_filename + \".csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zhMxo32fmnT"
      },
      "source": [
        "# **6. Pair Landsat with *in situ* data**\n",
        "This next section is optional, but allows you to compare any field data you may have to the Landsat data produced here. This section uses the CSV exported above.<p>\n",
        "In order for this to run successfully, your data must be in CSV format and have the following headers/columns:\n",
        "\n",
        "\n",
        "*  `datetime`: Date and time of each temperature reading in a recognized strftime format\n",
        "*  `temp_degC`: an integer or float number representing temperature in degrees Celcius\n",
        "*  `location`: for in-lake statistics, a column with lake zone names (string format, no special characters); can be all the same for a single output or differentiated by lake zones/sensors\n",
        "* `depth_m`: as an integer or float number, for statistics about the depth of sensors for matched scenes\n",
        "\n",
        "The file may have additional columns, but the above are mandatory fields. To see an example dataset, click here. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APg3jpUagGTf"
      },
      "source": [
        "## 6.1 Upload CSV of *insitu* data\n",
        "Choose an option below. For more ways to import data, see [here](https://colab.research.google.com/notebooks/io.ipynb)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P0Gg7oRYPGqF"
      },
      "outputs": [],
      "source": [
        "#@title Please provide some information about your data\n",
        "#@markdown What timezone is your data? <a href=\"https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\" target=\"_blank\">Olson Times Wikipedia Page</a> Specifically, you need to indicate the UTC offset for your data for proper pairing, including whether or not your data timestamp observes Daylight Savings Time. Enter the text for the matching UTC offset and DST behavior from the 'TZ database name' column in the linked table. Note that the sign of the GMT offset is intentionally inverted from the UTC offset.\n",
        "insitu_timezone = \"Etc/GMT+5\" #@param {type:\"string\"}\n",
        "#@markdown How is your datetime formatted? Please use <a href='https://strftime.org/' target='_blank'>strftime</a> format.\n",
        "datetime_format = \"%Y-%m-%d %H:%M:%S\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7X-Eyd1aFf7_"
      },
      "outputs": [],
      "source": [
        "#@title Option 1: Link to raw CSV from Github\n",
        "pasted_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "is_df = pd.read_csv(pasted_path)\n",
        "#is_df[\"datetime\"] = pd.to_datetime(is_df[\"datetime\"])\n",
        "print(\"dataframe created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oBcVtwu42-hG"
      },
      "outputs": [],
      "source": [
        "#@title Option 2: Upload CSV from your Google Drive folder\n",
        "#@markdown using the folder icon on the left hand side, navigate to the file in\n",
        "#@markdown your Google Drive (named 'drive' here), click on the three vertical dots to the right of the file\n",
        "#@markdown and click on 'copy file path'. Paste the copied path below.\n",
        "\n",
        "pasted_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "is_df = pd.read_csv(pasted_path)\n",
        "print(\"dataframe created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vHul1TwpD-go"
      },
      "outputs": [],
      "source": [
        "#@title Option 3: Upload CSV from your local file system to Colab\n",
        "#@markdown (this is temporary while connected to the current Colab runtime session)<br>\n",
        "#@markdown Run this cell to upload a file\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print(\"Paste this in the box below:\\n/content/{name}\".format(name=fn))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "52agyeVcKb8Z"
      },
      "outputs": [],
      "source": [
        "pasted_path = \"/content/insitu_temp_data_v2021-10-20.csv\" #@param {type:\"string\"}\n",
        "\n",
        "is_df = pd.read_csv(pasted_path)\n",
        "print(\"dataframe created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnJU34MEO1c-"
      },
      "source": [
        "## 6.2 Pair *insitu* data with Landsat data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xh155YwkBSlc"
      },
      "outputs": [],
      "source": [
        "#@title Convert local datetime to UTC time\n",
        "#@markdown __Run this block__ to apply conversion of your specified local time to UTC time. \n",
        "insitu_tz = pytz.timezone(insitu_timezone)\n",
        "landsat_tz = pytz.timezone(\"UTC\")\n",
        "\n",
        "def convert_datetime(dt, dtformat=datetime_format):\n",
        "  #converts string format insitu time to a datetime obj\n",
        "  dto = datetime.strptime(dt, datetime_format)\n",
        "  #makes it time aware\n",
        "  dto_local = insitu_tz.localize(dto)\n",
        "  #converts to utc\n",
        "  dto_utc = dto_local.astimezone(landsat_tz)\n",
        "  return dto_utc\n",
        "\n",
        "dtobj_series = is_df['datetime']\n",
        "dtobj_series_conv = dtobj_series.apply(convert_datetime)\n",
        "is_df['datetime_utc'] = dtobj_series_conv\n",
        "\n",
        "print(\"\\nDate/time function imported and datetime converted in dataframe\", strftime(\"%x %X\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O2csZXUp0Zlo"
      },
      "outputs": [],
      "source": [
        "#@markdown Enter the window of time (in minutes) from Landsat flyover where *insitu* data should be included. For example, `timewindow = 30` will include any data within 60 minutes of Landsat flyover (+/- 30 minutes)\n",
        "timewindow = 30 #@param {type:\"number\"\n",
        "\n",
        "cwd = output_dir\n",
        "os.chdir(cwd)\n",
        "\n",
        "outfile = (file_prefix + \"_temp_landsat_paired.csv\")\n",
        "\n",
        "print(f\"Time window: +- {timewindow} minutes\")\n",
        "print(f\"In-situ time Zone: {insitu_timezone}\")\n",
        "print(\"\\nLandsat input file:\\n\", os.path.join(cwd,temp_filename+\".csv\"))\n",
        "print(\"\\nOutput file will be saved to:\\n\", os.path.join(cwd,outfile))\n",
        "\n",
        "min_sec = timewindow * 60\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XYfj7ThEZsFl"
      },
      "outputs": [],
      "source": [
        "#@markdown The following codeblock compares the datasets and generate statistics \n",
        "#@markdown for in-lake data that matches Landsat flyovers.<br> \n",
        "#@markdown It also uses a subfolder called 'ancillary' (and creates the folder if needed) \n",
        "#@markdown that keeps a record of any insitu data points that are used for each Landsat scene.\n",
        "#@markdown If you have a large number of scenes to pair and/or a large dataframe of\n",
        "#@markdown in-situ data, this step may take some time. Timestamps that print below are \n",
        "#@markdown the Landast scene acquisition times where there are in-situ matches.\n",
        "\n",
        "print(\"Start:\", strftime(\"%x %X\"))\n",
        "file_name = (output_dir + '/' + temp_filename + '.csv')\n",
        "gee_csv = pd.read_csv(file_name)\n",
        "insitu_csv = is_df\n",
        "\n",
        "#make 'ancillary' folder in defined directory\n",
        "if not os.path.exists(os.path.join(cwd, 'ancillary')):\n",
        "    os.makedirs(os.path.join(cwd, 'ancillary'))\n",
        "print('Matched records files will be visible at ', cwd+ '/ancillary')\n",
        "\n",
        "#filter GEE output so that it's only as recent as minimum in-situ data\n",
        "insitumin = min(insitu_csv.datetime_utc)\n",
        "insitumax = max(insitu_csv.datetime_utc)\n",
        "\n",
        "def conv_lstime(i):\n",
        "  return pd.Timestamp(i, unit = 'ms', tz = 'utc')\n",
        "\n",
        "gee_csv['landsat_time_utc'] = gee_csv.datetime_landsat.apply(conv_lstime)\n",
        "\n",
        "gee_overlap = gee_csv[gee_csv.landsat_time_utc>insitumin]\n",
        "gee_overlap_fin = gee_overlap[gee_overlap.landsat_time_utc<insitumax]\n",
        "#reindex filtered dataset\n",
        "gee_overlap_fin = gee_overlap_fin.reset_index(drop=True)\n",
        "\n",
        "gee_datelist = gee_overlap_fin[\"landsat_time_utc\"]\n",
        "\n",
        "# function to convert dt_delta to seconds\n",
        "def delta_tosec(i):\n",
        "  total_secs = i.seconds + i.days*24*60*60\n",
        "  return total_secs\n",
        "\n",
        "for i in range(0,len(gee_datelist)):\n",
        "  scene = gee_overlap_fin['system:index'][i][-20:]\n",
        "  landsattime = gee_overlap_fin['landsat_time_utc'][i]\n",
        "\n",
        "  # create a df of insitu data that are observed within the user-specified cutoff \n",
        "\n",
        "  #calculate time delta for each obs\n",
        "  insitu_csv['dt_delta'] = landsattime - insitu_csv['datetime_utc']\n",
        "  insitu_csv['delta_secs'] = insitu_csv.dt_delta.apply(delta_tosec)  \n",
        "  same_time = insitu_csv[(abs(insitu_csv.delta_secs) <= min_sec)]\n",
        "  \n",
        "  #if the dataframe is not empty, summarize the results\n",
        "  if same_time.shape[0]>0:\n",
        "    print(landsattime)\n",
        "    gee_overlap_fin.loc[i, \"scene\"] = scene\n",
        "    gee_overlap_fin.loc[i, \"is_temp_avg\"] = same_time[\"temp_degC\"].mean()\n",
        "    gee_overlap_fin.loc[i, \"is_temp_stdev\"] = same_time[\"temp_degC\"].std()\n",
        "    gee_overlap_fin.loc[i, \"is_depth_avg\"] = same_time[\"depth_m\"].mean()\n",
        "    gee_overlap_fin.loc[i, \"is_depth_stdev\"] = same_time[\"depth_m\"].std()\n",
        "    gee_overlap_fin.loc[i, \"is_temp_med\"] = same_time[\"temp_degC\"].median()\n",
        "    gee_overlap_fin.loc[i, \"insitu_count\"] = same_time.shape[0]\n",
        "\n",
        "    site_stats = same_time.groupby(['location'])['temp_degC'].agg(['median', 'mean', 'std', 'count'])\n",
        "\n",
        "    sites = site_stats.axes[0]\n",
        "    stats = site_stats.axes[1]\n",
        "\n",
        "    for site in sites:\n",
        "        for stat in stats:\n",
        "            newcol = \"{0}_{1}\".format(str(site), str(stat))\n",
        "            gee_overlap_fin.loc[i, newcol] = site_stats[stat][site].item()\n",
        "\n",
        "    if same_time.shape[0] > 0:\n",
        "        same_time.to_csv(os.path.join(cwd, 'ancillary', scene + \".csv\"))\n",
        "\n",
        "out_csv = gee_overlap_fin[gee_overlap_fin[\"insitu_count\"] > 0]\n",
        "out_csv.to_csv(os.path.join(cwd, outfile))\n",
        "(\"Finished:\", strftime(\"%x %X\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz3nZJqn7R5G"
      },
      "source": [
        "##You are all set. Go analyze your lake surface temperature data! \n",
        "\n",
        "We suggest you save a copy of this notebook in the same folder as your downloaded data, which will appear in your Google Drive in the 'lakeCoSTR_output' folder for good measure.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DBZzaGgqpr8W"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}